{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKdcvMtSMpSg"
      },
      "source": [
        "#Майнор ИАД. Домашнее задание 3. YOLO.\n",
        "\n",
        "В этом задании вы напишете и обучите свой собственный YOLO детектор. Нужно будет разобраться со статьей: понять какого формата должна быть обучающая пара (x, y), как перевести лосс из математической формулы в питоновский код - ну и конечно понять и реализовать саму архитектуру модели.\n",
        "\n",
        "Выборка на котрой мы будем обучать модель состоит из разнообразных фотографий яблок, бананов и апельсинов. Данные скачиваем [отсюда](https://drive.google.com/file/d/1d8GSfZoWbraWCSUhX78yl4CnMFYE-5n3/view?usp=sharing).\n",
        "\n",
        "Баллы за ДЗ распределены следующим образом: \n",
        "- Выборка для YoloV1 - 2 балла\n",
        "- YOLO модель - 2 балла\n",
        "- YOLO Loss - 3 балла\n",
        "- Вспомогательные функции - 2 балла\n",
        "- Обучение и расчет метрик - 2 балла\n",
        "\n",
        "Для построения и обучения можно использовать как pytorch, так и pytorch-lightning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TY4niK1xMpSg",
        "outputId": "33a8cc89-e359-4cc4-eeb6-6caa82281c61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xmltodict\n",
            "  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-1.8.5.post0-py3-none-any.whl (800 kB)\n",
            "\u001b[K     |████████████████████████████████| 800 kB 14.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (21.3)\n",
            "Collecting lightning-utilities!=0.4.0,>=0.3.0\n",
            "  Downloading lightning_utilities-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (4.4.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (2022.11.0)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.13.0+cu116)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |████████████████████████████████| 512 kB 75.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (6.0)\n",
            "Collecting tensorboardX>=2.2\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 70.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (22.1.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch-lightning) (3.0.9)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=2.2->pytorch-lightning) (3.19.6)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.0.4)\n",
            "Installing collected packages: torchmetrics, tensorboardX, lightning-utilities, xmltodict, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.4.2 pytorch-lightning-1.8.5.post0 tensorboardX-2.5.1 torchmetrics-0.11.0 xmltodict-0.13.0\n"
          ]
        }
      ],
      "source": [
        "# Данная библиотека понадобится нам, чтобы обработать разметку\n",
        "! pip install xmltodict pytorch-lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNSQ7FNss30F"
      },
      "source": [
        "Скачаем данные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lN1dE6eY7PjV",
        "outputId": "5823a235-f1aa-439a-cce1-53a498a4c6dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12\n",
            "drwx------ 4 root root 4096 Nov 26 08:32 data\n",
            "drwxr-xr-x 3 root root 4096 Dec 19 18:25 __MACOSX\n",
            "drwxr-xr-x 1 root root 4096 Dec 16 21:15 sample_data\n"
          ]
        }
      ],
      "source": [
        "!wget --quiet --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://drive.google.com/uc?export=download&id=1d8GSfZoWbraWCSUhX78yl4CnMFYE-5n3' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1d8GSfZoWbraWCSUhX78yl4CnMFYE-5n3\" -O data.zip && rm -rf /tmp/cookies.txt\n",
        "!unzip -q data.zip\n",
        "!rm data.zip\n",
        "!ls -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqwaHl3ntBaN",
        "outputId": "dd2e6b11-890a-40dc-cb36-74afafb1d7d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<annotation>\n",
            "\t<folder>train</folder>\n",
            "\t<filename>apple_3.jpg</filename>\n",
            "\t<path>C:\\tensorflow1\\models\\research\\object_detection\\images\\train\\apple_3.jpg</path>\n",
            "\t<source>\n",
            "\t\t<database>Unknown</database>\n",
            "\t</source>\n",
            "\t<size>\n",
            "\t\t<width>1000</width>\n",
            "\t\t<height>708</height>\n",
            "\t\t<depth>3</depth>\n",
            "\t</size>\n",
            "\t<segmented>0</segmented>\n",
            "\t<object>\n",
            "\t\t<name>apple</name>\n",
            "\t\t<pose>Unspecified</pose>\n",
            "\t\t<truncated>1</truncated>\n",
            "\t\t<difficult>0</difficult>\n",
            "\t\t<bndbox>\n",
            "\t\t\t<xmin>584</xmin>\n",
            "\t\t\t<ymin>438</ymin>\n",
            "\t\t\t<xmax>867</xmax>\n",
            "\t\t\t<ymax>708</ymax>\n",
            "\t\t</bndbox>\n",
            "\t</object>\n",
            "\t<object>\n",
            "\t\t<name>apple</name>\n",
            "\t\t<pose>Unspecified</pose>\n",
            "\t\t<truncated>0</truncated>\n",
            "\t\t<difficult>0</difficult>\n",
            "\t\t<bndbox>\n",
            "\t\t\t<xmin>492</xmin>\n",
            "\t\t\t<ymin>141</ymin>\n",
            "\t\t\t<xmax>740</xmax>\n",
            "\t\t\t<ymax>394</ymax>\n",
            "\t\t</bndbox>\n",
            "\t</object>\n",
            "\t<object>\n",
            "\t\t<name>apple</name>\n",
            "\t\t<pose>Unspecified</pose>\n",
            "\t\t<truncated>0</truncated>\n",
            "\t\t<difficult>0</difficult>\n",
            "\t\t<bndbox>\n",
            "\t\t\t<xmin>176</xmin>\n",
            "\t\t\t<ymin>199</ymin>\n",
            "\t\t\t<xmax>490</xmax>\n",
            "\t\t\t<ymax>466</ymax>\n",
            "\t\t</bndbox>\n",
            "\t</object>\n",
            "\t<object>\n",
            "\t\t<name>apple</name>\n",
            "\t\t<pose>Unspecified</pose>\n",
            "\t\t<truncated>0</truncated>\n",
            "\t\t<difficult>0</difficult>\n",
            "\t\t<bndbox>\n",
            "\t\t\t<xmin>367</xmin>\n",
            "\t\t\t<ymin>17</ymin>\n",
            "\t\t\t<xmax>619</xmax>\n",
            "\t\t\t<ymax>240</ymax>\n",
            "\t\t</bndbox>\n",
            "\t</object>\n",
            "\t<object>\n",
            "\t\t<name>apple</name>\n",
            "\t\t<pose>Unspecified</pose>\n",
            "\t\t<truncated>0</truncated>\n",
            "\t\t<difficult>0</difficult>\n",
            "\t\t<bndbox>\n",
            "\t\t\t<xmin>642</xmin>\n",
            "\t\t\t<ymin>35</ymin>\n",
            "\t\t\t<xmax>907</xmax>\n",
            "\t\t\t<ymax>269</ymax>\n",
            "\t\t</bndbox>\n",
            "\t</object>\n",
            "</annotation>\n"
          ]
        }
      ],
      "source": [
        "!cat data/train/apple_3.xml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdQxrA5_MpSg"
      },
      "source": [
        "## Релизуйте выборку для YoloV1 - 2 балла"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXG9reop-BkS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import glob\n",
        "import tqdm\n",
        "import xmltodict\n",
        "import math\n",
        "\n",
        "from IPython.core.display import struct\n",
        "\n",
        "from typing import List\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "import albumentations as A\n",
        "import albumentations.pytorch\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import auc\n",
        "# Добавьте необходимые вам библиотеки, если их не окажется в списке выше"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "5y57mzDmL_Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install xml_to_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qnG5nsxmdWF",
        "outputId": "291a857a-4578-4bcb-c5db-c596c7cc7b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xml_to_dict\n",
            "  Downloading xml_to_dict-0.1.6-py3-none-any.whl (3.6 kB)\n",
            "Installing collected packages: xml-to-dict\n",
            "Successfully installed xml-to-dict-0.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rd88hnZiMpSh"
      },
      "outputs": [],
      "source": [
        "def intersection_over_union(predicted_bbox, gt_bbox) -> float:\n",
        "    \"\"\"\n",
        "    Intersection Over Union для двух прямоугольников\n",
        "\n",
        "    :param: predicted_bbox - [x_min, y_min, x_max, y_max]\n",
        "    :param: gt_bbox - [x_min, y_min, x_max, y_max]\n",
        "    \n",
        "    :return: Intersection Over Union\n",
        "    \"\"\"\n",
        "\n",
        "    intersection_bbox = np.array(\n",
        "        [\n",
        "            max(predicted_bbox[0], gt_bbox[0]),\n",
        "            max(predicted_bbox[1], gt_bbox[1]),\n",
        "            min(predicted_bbox[2], gt_bbox[2]),\n",
        "            min(predicted_bbox[3], gt_bbox[3]),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    intersection_area = max(intersection_bbox[2] - intersection_bbox[0], 0) * max(\n",
        "        intersection_bbox[3] - intersection_bbox[1], 0\n",
        "    )\n",
        "    area_dt = (predicted_bbox[2] - predicted_bbox[0]) * (predicted_bbox[3] - predicted_bbox[1])\n",
        "    area_gt = (gt_bbox[2] - gt_bbox[0]) * (gt_bbox[3] - gt_bbox[1])\n",
        "\n",
        "    union_area = area_dt + area_gt - intersection_area\n",
        "\n",
        "    iou = intersection_area / union_area\n",
        "    return iou"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from xml_to_dict import XMLtoDict"
      ],
      "metadata": {
        "id": "zy44L94VmtxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjZkU0vzMpSh"
      },
      "outputs": [],
      "source": [
        "class2tag = {\"apple\": 1, \"orange\": 2, \"banana\": 3}\n",
        "\n",
        "class FruitDataset(Dataset):\n",
        "    def __init__(self, data_dir, transforms=None):\n",
        "        self.image_paths = glob.glob(os.path.join(data_dir, \"*.jpg\")) ## YOUR CODE\n",
        "        self.image_paths.sort()\n",
        "        self.box_paths = (glob.glob(os.path.join(data_dir, \"*.xml\"))) ## YOUR CODE\n",
        "        self.box_paths.sort() #https://github.com/hse-ds/iad-deep-learning/blob/master/2022/seminars/sem06/sem06_solved.ipynb\n",
        "        print(self.box_paths)\n",
        "        print(self.image_paths)\n",
        "        assert len(self.image_paths) == len(self.box_paths)\n",
        "\n",
        "        self.transforms = transforms\n",
        "\n",
        "    # Координаты прямоугольников советуем вернуть именно в формате (x_center, y_center, width, height)\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        image = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\")) #https://github.com/hse-ds/iad-deep-learning/blob/master/2022/seminars/sem06/sem06_solved.ipynb\n",
        "        print(image.shape)\n",
        "        im_w = image.shape[1]\n",
        "        im_h = image.shape[0]\n",
        "        boxes, class_labels = self.__get_boxes_from_xml(self.box_paths[idx], im_w, im_h)\n",
        "\n",
        "        if self.transforms:\n",
        "          transformed = self.transforms(image=image, bboxes=boxes, class_labels=class_labels)\n",
        "          transformed_image = transformed['image']\n",
        "          transformed_bboxes = transformed['bboxes']\n",
        "          transformed_class_labels = transformed['class_labels'] #https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/\n",
        "\n",
        "        target_tensor = np.zeros((7,7,13))\n",
        "\n",
        "        for bbox_id in range(len(transformed_bboxes)):\n",
        "          bbox = transformed_bboxes[bbox_id]\n",
        "          for l in range(7):\n",
        "            for c in range(7):\n",
        "              if (bbox[0] >= l/7) and (bbox[0] <= 2*l/7) and (bbox[1] >= c/7) and (bbox[1] <= 2*c/7):\n",
        "                if target_tensor[l][c][0] == 0:\n",
        "                  target_tensor[l][c][0] = bbox[0] - (l/7) * 7\n",
        "                  target_tensor[l][c][1] = bbox[1] - (c/7) * 7\n",
        "                  target_tensor[l][c][2] = bbox[2]\n",
        "                  target_tensor[l][c][3] = bbox[3]\n",
        "                  target_tensor[l][c][4] = 1.0\n",
        "                  cell = torch.tensor([l/7, c/7, (l + 1)/7, (c + 1)/7])\n",
        "                  obj = torch.tensor([bbox[0] - (0.5 * bbox[2]), bbox[1] - (0.5 * bbox[3]), bbox[0] + (0.5 * bbox[2]), bbox[1] + (0.5 * bbox[3])])\n",
        "                  confidence = intersection_over_union(cell, obj)\n",
        "                  target_tensor[l][c][4] = confidence\n",
        "                  \n",
        "                  break\n",
        "                else:\n",
        "                  target_tensor[l][c][0] = bbox[0] - (l/7) * 7\n",
        "                  target_tensor[l][c][1] = bbox[1] - (c/7) * 7\n",
        "                  target_tensor[l][c][7] = bbox[2]\n",
        "                  target_tensor[l][c][8] = bbox[3]\n",
        "                  cell = torch.tensor([l/7, c/7, (l + 1)/7, (c + 1)/7])\n",
        "                  obj = torch.tensor([bbox[0] - (0.5 * bbox[2]), bbox[1] - (0.5 * bbox[3]), bbox[0] + (0.5 * bbox[2]), bbox[1] + (0.5 * bbox[3])])\n",
        "                  confidence = intersection_over_union(cell, obj)\n",
        "                  target_tensor[l][c][4] = confidence\n",
        "\n",
        "                  break\n",
        "                target_tensor[l][c][9+class_labels[bbox_id]] = 1.0\n",
        "                print('bbox1', bbox[0] - (l/7) * 7, bbox[1] - (col/7) * 7, l, c, bbox)\n",
        "                \n",
        "                break\n",
        "\n",
        "        return torch.tensor(np.swapaxes(image, 0, 2), dtype=float)/255.0, torch.tensor(target_tensor, dtype=float)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "\n",
        "    def __get_boxes_from_xml(self, xml_filename: str, im_w, im_h):\n",
        "      \"\"\"\n",
        "      Метод, который считает и распарсит (с помощью xmltodict) переданный xml\n",
        "      файл и вернет координаты прямоугольников обьектов на соответсвующей фотографии\n",
        "      и название класса обьекта в каждом прямоугольнике\n",
        "\n",
        "      Обратите внимание, что обьектов может быть как несколько, так и один единственный\n",
        "      \"\"\"\n",
        "      boxes = []\n",
        "      class_labels = []\n",
        "\n",
        "      parsed_xml = xmltodict.parse(open(xml_filename).read())\n",
        "\n",
        "      if type(parsed_xml.get(\"annotation\").get(\"object\")) == list:\n",
        "        for n in parsed_xml.get(\"annotation\").get(\"object\"):\n",
        "          boxes.append(self.__convert_to_yolo_box_params([float(n.get('bndbox').get(\"xmin\")), float(n.get('bndbox').get(\"ymin\")), float(n.get('bndbox').get(\"xmax\")), float(n.get('bndbox').get(\"xmax\")), float(n.get(\"name\"))], im_w, im_h))\n",
        "          class_labels.append(n.get(\"name\"))\n",
        "      else:\n",
        "        boxes.append(self.__convert_to_yolo_box_params([float(parsed_xml.get('annotation').get('object').get('bndbox').get('xmin')), float(parsed_xml.get('annotation').get('object').get('bndbox').get('ymin')), float(parsed_xml.get('annotation').get('object').get('bndbox').get('xmax')), float(parsed_xml.get('annotation').get('object').get('bndbox').get('ymax'))], im_w, im_h))\n",
        "        class_labels.append(parsed_xml.get('annotation').get('object').get('bndbox').get('name'))\n",
        "\n",
        "      print(boxes)  \n",
        "      return boxes, class_labels\n",
        "\n",
        "    def __convert_to_yolo_box_params(self, box_coordinates: List[int], im_w, im_h):\n",
        "      \"\"\"\n",
        "      Перейти от [xmin, ymin, xmax, ymax] к [x_center, y_center, width, height].\n",
        "      \n",
        "      Обратите внимание, что параметры [x_center, y_center, width, height] - это\n",
        "      относительные значение в отрезке [0, 1]\n",
        "\n",
        "      :param: box_coordinates - координаты коробки в формате [xmin, ymin, xmax, ymax]\n",
        "      :param: im_w - ширина исходного изображения\n",
        "      :param: im_h - высота исходного изображения\n",
        "\n",
        "      :return: координаты коробки в формате [x_center, y_center, width, height]\n",
        "      \"\"\"\n",
        "\n",
        "      ans = []\n",
        "\n",
        "      ans.append((box_coordinates[0] + box_coordinates[2]) / 2 / im_w)  # x_center\n",
        "      ans.append((box_coordinates[1] + box_coordinates[3]) / 2 / im_h)  # y_center\n",
        "      \n",
        "      ans.append((box_coordinates[2] - box_coordinates[0]) / im_w)  # width\n",
        "      ans.append((box_coordinates[3] - box_coordinates[1]) / im_h)  # height\n",
        "      return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwXeSiAjdGeq"
      },
      "outputs": [],
      "source": [
        "WIDTH = 448\n",
        "HEIGHT = 448\n",
        "\n",
        "train_transform = A.Compose([A.Resize(height=HEIGHT, width=WIDTH),\n",
        "                             A.pytorch.transforms.ToTensorV2()],\n",
        "                             bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
        "\n",
        "test_transform = A.Compose([A.Resize(height=448, width=448),\n",
        "                            A.pytorch.transforms.ToTensorV2()],\n",
        "                            bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels'])) #https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayPwbRKocdCE",
        "outputId": "64cef694-2cde-4ef9-97ba-87e0b3639c93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['./data/train/apple_1.xml', './data/train/apple_10.xml', './data/train/apple_11.xml', './data/train/apple_12.xml', './data/train/apple_13.xml', './data/train/apple_14.xml', './data/train/apple_15.xml', './data/train/apple_16.xml', './data/train/apple_17.xml', './data/train/apple_18.xml', './data/train/apple_19.xml', './data/train/apple_2.xml', './data/train/apple_20.xml', './data/train/apple_21.xml', './data/train/apple_22.xml', './data/train/apple_23.xml', './data/train/apple_24.xml', './data/train/apple_25.xml', './data/train/apple_26.xml', './data/train/apple_27.xml', './data/train/apple_28.xml', './data/train/apple_29.xml', './data/train/apple_3.xml', './data/train/apple_30.xml', './data/train/apple_31.xml', './data/train/apple_32.xml', './data/train/apple_33.xml', './data/train/apple_35.xml', './data/train/apple_36.xml', './data/train/apple_37.xml', './data/train/apple_38.xml', './data/train/apple_39.xml', './data/train/apple_4.xml', './data/train/apple_40.xml', './data/train/apple_41.xml', './data/train/apple_42.xml', './data/train/apple_43.xml', './data/train/apple_44.xml', './data/train/apple_45.xml', './data/train/apple_46.xml', './data/train/apple_47.xml', './data/train/apple_48.xml', './data/train/apple_49.xml', './data/train/apple_5.xml', './data/train/apple_50.xml', './data/train/apple_51.xml', './data/train/apple_52.xml', './data/train/apple_53.xml', './data/train/apple_54.xml', './data/train/apple_55.xml', './data/train/apple_56.xml', './data/train/apple_57.xml', './data/train/apple_58.xml', './data/train/apple_59.xml', './data/train/apple_6.xml', './data/train/apple_60.xml', './data/train/apple_61.xml', './data/train/apple_62.xml', './data/train/apple_63.xml', './data/train/apple_64.xml', './data/train/apple_65.xml', './data/train/apple_66.xml', './data/train/apple_67.xml', './data/train/apple_68.xml', './data/train/apple_69.xml', './data/train/apple_7.xml', './data/train/apple_70.xml', './data/train/apple_71.xml', './data/train/apple_72.xml', './data/train/apple_73.xml', './data/train/apple_74.xml', './data/train/apple_75.xml', './data/train/apple_76.xml', './data/train/apple_8.xml', './data/train/apple_9.xml', './data/train/banana_1.xml', './data/train/banana_10.xml', './data/train/banana_11.xml', './data/train/banana_12.xml', './data/train/banana_13.xml', './data/train/banana_14.xml', './data/train/banana_16.xml', './data/train/banana_17.xml', './data/train/banana_2.xml', './data/train/banana_20.xml', './data/train/banana_21.xml', './data/train/banana_22.xml', './data/train/banana_23.xml', './data/train/banana_24.xml', './data/train/banana_25.xml', './data/train/banana_26.xml', './data/train/banana_27.xml', './data/train/banana_28.xml', './data/train/banana_29.xml', './data/train/banana_3.xml', './data/train/banana_30.xml', './data/train/banana_31.xml', './data/train/banana_32.xml', './data/train/banana_33.xml', './data/train/banana_34.xml', './data/train/banana_35.xml', './data/train/banana_36.xml', './data/train/banana_37.xml', './data/train/banana_38.xml', './data/train/banana_39.xml', './data/train/banana_4.xml', './data/train/banana_40.xml', './data/train/banana_41.xml', './data/train/banana_42.xml', './data/train/banana_43.xml', './data/train/banana_44.xml', './data/train/banana_45.xml', './data/train/banana_46.xml', './data/train/banana_47.xml', './data/train/banana_48.xml', './data/train/banana_49.xml', './data/train/banana_5.xml', './data/train/banana_50.xml', './data/train/banana_51.xml', './data/train/banana_52.xml', './data/train/banana_53.xml', './data/train/banana_54.xml', './data/train/banana_55.xml', './data/train/banana_56.xml', './data/train/banana_57.xml', './data/train/banana_58.xml', './data/train/banana_59.xml', './data/train/banana_6.xml', './data/train/banana_60.xml', './data/train/banana_61.xml', './data/train/banana_62.xml', './data/train/banana_63.xml', './data/train/banana_64.xml', './data/train/banana_65.xml', './data/train/banana_66.xml', './data/train/banana_67.xml', './data/train/banana_68.xml', './data/train/banana_69.xml', './data/train/banana_7.xml', './data/train/banana_70.xml', './data/train/banana_71.xml', './data/train/banana_72.xml', './data/train/banana_73.xml', './data/train/banana_74.xml', './data/train/banana_75.xml', './data/train/banana_76.xml', './data/train/banana_8.xml', './data/train/banana_9.xml', './data/train/mixed_1.xml', './data/train/mixed_10.xml', './data/train/mixed_11.xml', './data/train/mixed_12.xml', './data/train/mixed_13.xml', './data/train/mixed_14.xml', './data/train/mixed_15.xml', './data/train/mixed_16.xml', './data/train/mixed_17.xml', './data/train/mixed_18.xml', './data/train/mixed_19.xml', './data/train/mixed_2.xml', './data/train/mixed_20.xml', './data/train/mixed_3.xml', './data/train/mixed_4.xml', './data/train/mixed_5.xml', './data/train/mixed_6.xml', './data/train/mixed_7.xml', './data/train/mixed_8.xml', './data/train/mixed_9.xml', './data/train/orange_1.xml', './data/train/orange_10.xml', './data/train/orange_11.xml', './data/train/orange_12.xml', './data/train/orange_13.xml', './data/train/orange_14.xml', './data/train/orange_15.xml', './data/train/orange_16.xml', './data/train/orange_17.xml', './data/train/orange_18.xml', './data/train/orange_19.xml', './data/train/orange_2.xml', './data/train/orange_20.xml', './data/train/orange_21.xml', './data/train/orange_22.xml', './data/train/orange_23.xml', './data/train/orange_24.xml', './data/train/orange_25.xml', './data/train/orange_26.xml', './data/train/orange_27.xml', './data/train/orange_28.xml', './data/train/orange_29.xml', './data/train/orange_3.xml', './data/train/orange_30.xml', './data/train/orange_31.xml', './data/train/orange_32.xml', './data/train/orange_33.xml', './data/train/orange_34.xml', './data/train/orange_35.xml', './data/train/orange_36.xml', './data/train/orange_37.xml', './data/train/orange_38.xml', './data/train/orange_4.xml', './data/train/orange_40.xml', './data/train/orange_41.xml', './data/train/orange_42.xml', './data/train/orange_43.xml', './data/train/orange_44.xml', './data/train/orange_46.xml', './data/train/orange_47.xml', './data/train/orange_48.xml', './data/train/orange_49.xml', './data/train/orange_5.xml', './data/train/orange_50.xml', './data/train/orange_51.xml', './data/train/orange_52.xml', './data/train/orange_53.xml', './data/train/orange_54.xml', './data/train/orange_55.xml', './data/train/orange_56.xml', './data/train/orange_57.xml', './data/train/orange_58.xml', './data/train/orange_59.xml', './data/train/orange_6.xml', './data/train/orange_60.xml', './data/train/orange_61.xml', './data/train/orange_62.xml', './data/train/orange_63.xml', './data/train/orange_64.xml', './data/train/orange_67.xml', './data/train/orange_68.xml', './data/train/orange_69.xml', './data/train/orange_7.xml', './data/train/orange_70.xml', './data/train/orange_71.xml', './data/train/orange_72.xml', './data/train/orange_73.xml', './data/train/orange_74.xml', './data/train/orange_75.xml', './data/train/orange_76.xml', './data/train/orange_8.xml', './data/train/orange_9.xml']\n",
            "['./data/train/apple_1.jpg', './data/train/apple_10.jpg', './data/train/apple_11.jpg', './data/train/apple_12.jpg', './data/train/apple_13.jpg', './data/train/apple_14.jpg', './data/train/apple_15.jpg', './data/train/apple_16.jpg', './data/train/apple_17.jpg', './data/train/apple_18.jpg', './data/train/apple_19.jpg', './data/train/apple_2.jpg', './data/train/apple_20.jpg', './data/train/apple_21.jpg', './data/train/apple_22.jpg', './data/train/apple_23.jpg', './data/train/apple_24.jpg', './data/train/apple_25.jpg', './data/train/apple_26.jpg', './data/train/apple_27.jpg', './data/train/apple_28.jpg', './data/train/apple_29.jpg', './data/train/apple_3.jpg', './data/train/apple_30.jpg', './data/train/apple_31.jpg', './data/train/apple_32.jpg', './data/train/apple_33.jpg', './data/train/apple_35.jpg', './data/train/apple_36.jpg', './data/train/apple_37.jpg', './data/train/apple_38.jpg', './data/train/apple_39.jpg', './data/train/apple_4.jpg', './data/train/apple_40.jpg', './data/train/apple_41.jpg', './data/train/apple_42.jpg', './data/train/apple_43.jpg', './data/train/apple_44.jpg', './data/train/apple_45.jpg', './data/train/apple_46.jpg', './data/train/apple_47.jpg', './data/train/apple_48.jpg', './data/train/apple_49.jpg', './data/train/apple_5.jpg', './data/train/apple_50.jpg', './data/train/apple_51.jpg', './data/train/apple_52.jpg', './data/train/apple_53.jpg', './data/train/apple_54.jpg', './data/train/apple_55.jpg', './data/train/apple_56.jpg', './data/train/apple_57.jpg', './data/train/apple_58.jpg', './data/train/apple_59.jpg', './data/train/apple_6.jpg', './data/train/apple_60.jpg', './data/train/apple_61.jpg', './data/train/apple_62.jpg', './data/train/apple_63.jpg', './data/train/apple_64.jpg', './data/train/apple_65.jpg', './data/train/apple_66.jpg', './data/train/apple_67.jpg', './data/train/apple_68.jpg', './data/train/apple_69.jpg', './data/train/apple_7.jpg', './data/train/apple_70.jpg', './data/train/apple_71.jpg', './data/train/apple_72.jpg', './data/train/apple_73.jpg', './data/train/apple_74.jpg', './data/train/apple_75.jpg', './data/train/apple_76.jpg', './data/train/apple_8.jpg', './data/train/apple_9.jpg', './data/train/banana_1.jpg', './data/train/banana_10.jpg', './data/train/banana_11.jpg', './data/train/banana_12.jpg', './data/train/banana_13.jpg', './data/train/banana_14.jpg', './data/train/banana_16.jpg', './data/train/banana_17.jpg', './data/train/banana_2.jpg', './data/train/banana_20.jpg', './data/train/banana_21.jpg', './data/train/banana_22.jpg', './data/train/banana_23.jpg', './data/train/banana_24.jpg', './data/train/banana_25.jpg', './data/train/banana_26.jpg', './data/train/banana_27.jpg', './data/train/banana_28.jpg', './data/train/banana_29.jpg', './data/train/banana_3.jpg', './data/train/banana_30.jpg', './data/train/banana_31.jpg', './data/train/banana_32.jpg', './data/train/banana_33.jpg', './data/train/banana_34.jpg', './data/train/banana_35.jpg', './data/train/banana_36.jpg', './data/train/banana_37.jpg', './data/train/banana_38.jpg', './data/train/banana_39.jpg', './data/train/banana_4.jpg', './data/train/banana_40.jpg', './data/train/banana_41.jpg', './data/train/banana_42.jpg', './data/train/banana_43.jpg', './data/train/banana_44.jpg', './data/train/banana_45.jpg', './data/train/banana_46.jpg', './data/train/banana_47.jpg', './data/train/banana_48.jpg', './data/train/banana_49.jpg', './data/train/banana_5.jpg', './data/train/banana_50.jpg', './data/train/banana_51.jpg', './data/train/banana_52.jpg', './data/train/banana_53.jpg', './data/train/banana_54.jpg', './data/train/banana_55.jpg', './data/train/banana_56.jpg', './data/train/banana_57.jpg', './data/train/banana_58.jpg', './data/train/banana_59.jpg', './data/train/banana_6.jpg', './data/train/banana_60.jpg', './data/train/banana_61.jpg', './data/train/banana_62.jpg', './data/train/banana_63.jpg', './data/train/banana_64.jpg', './data/train/banana_65.jpg', './data/train/banana_66.jpg', './data/train/banana_67.jpg', './data/train/banana_68.jpg', './data/train/banana_69.jpg', './data/train/banana_7.jpg', './data/train/banana_70.jpg', './data/train/banana_71.jpg', './data/train/banana_72.jpg', './data/train/banana_73.jpg', './data/train/banana_74.jpg', './data/train/banana_75.jpg', './data/train/banana_76.jpg', './data/train/banana_8.jpg', './data/train/banana_9.jpg', './data/train/mixed_1.jpg', './data/train/mixed_10.jpg', './data/train/mixed_11.jpg', './data/train/mixed_12.jpg', './data/train/mixed_13.jpg', './data/train/mixed_14.jpg', './data/train/mixed_15.jpg', './data/train/mixed_16.jpg', './data/train/mixed_17.jpg', './data/train/mixed_18.jpg', './data/train/mixed_19.jpg', './data/train/mixed_2.jpg', './data/train/mixed_20.jpg', './data/train/mixed_3.jpg', './data/train/mixed_4.jpg', './data/train/mixed_5.jpg', './data/train/mixed_6.jpg', './data/train/mixed_7.jpg', './data/train/mixed_8.jpg', './data/train/mixed_9.jpg', './data/train/orange_1.jpg', './data/train/orange_10.jpg', './data/train/orange_11.jpg', './data/train/orange_12.jpg', './data/train/orange_13.jpg', './data/train/orange_14.jpg', './data/train/orange_15.jpg', './data/train/orange_16.jpg', './data/train/orange_17.jpg', './data/train/orange_18.jpg', './data/train/orange_19.jpg', './data/train/orange_2.jpg', './data/train/orange_20.jpg', './data/train/orange_21.jpg', './data/train/orange_22.jpg', './data/train/orange_23.jpg', './data/train/orange_24.jpg', './data/train/orange_25.jpg', './data/train/orange_26.jpg', './data/train/orange_27.jpg', './data/train/orange_28.jpg', './data/train/orange_29.jpg', './data/train/orange_3.jpg', './data/train/orange_30.jpg', './data/train/orange_31.jpg', './data/train/orange_32.jpg', './data/train/orange_33.jpg', './data/train/orange_34.jpg', './data/train/orange_35.jpg', './data/train/orange_36.jpg', './data/train/orange_37.jpg', './data/train/orange_38.jpg', './data/train/orange_4.jpg', './data/train/orange_40.jpg', './data/train/orange_41.jpg', './data/train/orange_42.jpg', './data/train/orange_43.jpg', './data/train/orange_44.jpg', './data/train/orange_46.jpg', './data/train/orange_47.jpg', './data/train/orange_48.jpg', './data/train/orange_49.jpg', './data/train/orange_5.jpg', './data/train/orange_50.jpg', './data/train/orange_51.jpg', './data/train/orange_52.jpg', './data/train/orange_53.jpg', './data/train/orange_54.jpg', './data/train/orange_55.jpg', './data/train/orange_56.jpg', './data/train/orange_57.jpg', './data/train/orange_58.jpg', './data/train/orange_59.jpg', './data/train/orange_6.jpg', './data/train/orange_60.jpg', './data/train/orange_61.jpg', './data/train/orange_62.jpg', './data/train/orange_63.jpg', './data/train/orange_64.jpg', './data/train/orange_67.jpg', './data/train/orange_68.jpg', './data/train/orange_69.jpg', './data/train/orange_7.jpg', './data/train/orange_70.jpg', './data/train/orange_71.jpg', './data/train/orange_72.jpg', './data/train/orange_73.jpg', './data/train/orange_74.jpg', './data/train/orange_75.jpg', './data/train/orange_76.jpg', './data/train/orange_8.jpg', './data/train/orange_9.jpg']\n",
            "['./data/test/apple_77.xml', './data/test/apple_78.xml', './data/test/apple_79.xml', './data/test/apple_80.xml', './data/test/apple_81.xml', './data/test/apple_82.xml', './data/test/apple_83.xml', './data/test/apple_84.xml', './data/test/apple_85.xml', './data/test/apple_86.xml', './data/test/apple_87.xml', './data/test/apple_88.xml', './data/test/apple_89.xml', './data/test/apple_90.xml', './data/test/apple_91.xml', './data/test/apple_92.xml', './data/test/apple_93.xml', './data/test/apple_94.xml', './data/test/apple_95.xml', './data/test/banana_77.xml', './data/test/banana_78.xml', './data/test/banana_79.xml', './data/test/banana_80.xml', './data/test/banana_81.xml', './data/test/banana_82.xml', './data/test/banana_83.xml', './data/test/banana_84.xml', './data/test/banana_85.xml', './data/test/banana_86.xml', './data/test/banana_87.xml', './data/test/banana_88.xml', './data/test/banana_89.xml', './data/test/banana_90.xml', './data/test/banana_91.xml', './data/test/banana_92.xml', './data/test/banana_93.xml', './data/test/banana_94.xml', './data/test/mixed_21.xml', './data/test/mixed_22.xml', './data/test/mixed_23.xml', './data/test/mixed_24.xml', './data/test/mixed_25.xml', './data/test/orange_77.xml', './data/test/orange_78.xml', './data/test/orange_79.xml', './data/test/orange_80.xml', './data/test/orange_81.xml', './data/test/orange_82.xml', './data/test/orange_83.xml', './data/test/orange_84.xml', './data/test/orange_85.xml', './data/test/orange_86.xml', './data/test/orange_87.xml', './data/test/orange_89.xml', './data/test/orange_90.xml', './data/test/orange_91.xml', './data/test/orange_92.xml', './data/test/orange_93.xml', './data/test/orange_94.xml', './data/test/orange_95.xml']\n",
            "['./data/test/apple_77.jpg', './data/test/apple_78.jpg', './data/test/apple_79.jpg', './data/test/apple_80.jpg', './data/test/apple_81.jpg', './data/test/apple_82.jpg', './data/test/apple_83.jpg', './data/test/apple_84.jpg', './data/test/apple_85.jpg', './data/test/apple_86.jpg', './data/test/apple_87.jpg', './data/test/apple_88.jpg', './data/test/apple_89.jpg', './data/test/apple_90.jpg', './data/test/apple_91.jpg', './data/test/apple_92.jpg', './data/test/apple_93.jpg', './data/test/apple_94.jpg', './data/test/apple_95.jpg', './data/test/banana_77.jpg', './data/test/banana_78.jpg', './data/test/banana_79.jpg', './data/test/banana_80.jpg', './data/test/banana_81.jpg', './data/test/banana_82.jpg', './data/test/banana_83.jpg', './data/test/banana_84.jpg', './data/test/banana_85.jpg', './data/test/banana_86.jpg', './data/test/banana_87.jpg', './data/test/banana_88.jpg', './data/test/banana_89.jpg', './data/test/banana_90.jpg', './data/test/banana_91.jpg', './data/test/banana_92.jpg', './data/test/banana_93.jpg', './data/test/banana_94.jpg', './data/test/mixed_21.jpg', './data/test/mixed_22.jpg', './data/test/mixed_23.jpg', './data/test/mixed_24.jpg', './data/test/mixed_25.jpg', './data/test/orange_77.jpg', './data/test/orange_78.jpg', './data/test/orange_79.jpg', './data/test/orange_80.jpg', './data/test/orange_81.jpg', './data/test/orange_82.jpg', './data/test/orange_83.jpg', './data/test/orange_84.jpg', './data/test/orange_85.jpg', './data/test/orange_86.jpg', './data/test/orange_87.jpg', './data/test/orange_89.jpg', './data/test/orange_90.jpg', './data/test/orange_91.jpg', './data/test/orange_92.jpg', './data/test/orange_93.jpg', './data/test/orange_94.jpg', './data/test/orange_95.jpg']\n"
          ]
        }
      ],
      "source": [
        "train_dataset = FruitDataset(\n",
        "    transforms=train_transform,\n",
        "    data_dir=\"./data/train/\"\n",
        "    )\n",
        "\n",
        "val_dataset = FruitDataset(\n",
        "    transforms=test_transform, \n",
        "    data_dir=\"./data/test/\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Немного проверок, чтобы убедиться в правильности направления решения\n",
        "assert isinstance(train_dataset[0], tuple)\n",
        "assert len(train_dataset[0]) == 2\n",
        "assert isinstance(train_dataset[0][0], torch.Tensor)\n",
        "print(\"Тесты успешно пройдены\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gQJwQ3fIQWG",
        "outputId": "a83ab26e-cf87-4985-cbd0-fcd9053aaa64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(349, 349, 3)\n",
            "[[0.48567335243553006, 0.5214899713467048, 0.9255014326647565, 0.9570200573065902]]\n",
            "(349, 349, 3)\n",
            "[[0.48567335243553006, 0.5214899713467048, 0.9255014326647565, 0.9570200573065902]]\n",
            "(349, 349, 3)\n",
            "[[0.48567335243553006, 0.5214899713467048, 0.9255014326647565, 0.9570200573065902]]\n",
            "Тесты успешно пройдены\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9V1Tl_GAdeIv"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size= 4,\n",
        "    shuffle=True)\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=4, \n",
        "    shuffle=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVJWo3xbMpSh"
      },
      "source": [
        "Теперь начинается основная часть домашнего задания: обучите модель YOLO для object detection на __обучающем__ датасете. \n",
        "\n",
        " - Создайте модель и функцию ошибки YoloV1 прочитав [оригинальную статью](https://paperswithcode.com/paper/you-only-look-once-unified-real-time-object)\n",
        " - Напишите функцию обучения модели\n",
        " - Используйте аугментации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxfMVwzHW2MJ"
      },
      "source": [
        "## Реализуйте Модель - 2 балла"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our network has 24\n",
        "convolutional layers followed by 2 fully connected layers.\n"
      ],
      "metadata": {
        "id": "GxIn34XjVb5R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PJwrvcWW1n7"
      },
      "outputs": [],
      "source": [
        "class CNNBlock(nn.Module):  # можно поменять на Lightning\n",
        "    def __init__(self, in_channels, out_channels, is_max_pool:bool=False, **kwargs):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels, dtype=float)  # в статье еще не знали про батчнорм, но мы то из будущего ...\n",
        "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
        "\n",
        "        self.is_maxpool = is_max_pool  # не после каждой свертки нужно делать maxpool\n",
        "        self.maxpool = nn.MaxPool2d(2, stride=2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.leakyrelu(self.batchnorm(self.conv(x)))\n",
        "\n",
        "        if self.is_maxpool:\n",
        "            x = self.maxpool(x)\n",
        "\n",
        "        return x\n",
        "    \n",
        "class YOLO(nn.Module):\n",
        "    def __init__(self, S=7, B=2, C=3):\n",
        "        \"\"\"\n",
        "        :param: S * S - количество ячеек на которые разбивается изображение\n",
        "        :param: B - количество предсказанных прямоугольников в каждой ячейке\n",
        "        :param: C - количество классов\n",
        "        \"\"\"\n",
        "        \n",
        "        super(YOLO, self).__init__()\n",
        "\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "\n",
        "        self.con1 = CNNBlock(in_channels=3, out_channels=64, kernel_size=(7,7), is_max_pool=True, stride=2, dtype=float)\n",
        "\n",
        "        self.con2 = CNNBlock(in_channels=64, out_channels=192, kernel_size=(3,3), is_max_pool=True, dtype=float)\n",
        "\n",
        "        self.con3 = nn.Sequential(\n",
        "            CNNBlock(in_channels=192, out_channels=128, kernel_size=(1,1), is_max_pool=False, dtype=float),\n",
        "            CNNBlock(in_channels=128, out_channels=256, kernel_size=(3,3), is_max_pool=False, dtype=float),\n",
        "            CNNBlock(in_channels=256, out_channels=256, kernel_size=(1,1), is_max_pool=False, dtype=float),\n",
        "            CNNBlock(in_channels=256, out_channels=512, kernel_size=(3,3), is_max_pool=True, dtype=float)\n",
        "            )\n",
        "        \n",
        "        self.con4 = nn.Sequential(\n",
        "            CNNBlock(in_channels=512, out_channels=256, kernel_size=(1,1), is_max_pool=False, dtype=torch.float64),\n",
        "            CNNBlock(in_channels=256, out_channels=512, kernel_size=(3,3), is_max_pool=False, dtype=torch.float64),\n",
        "            CNNBlock(in_channels=512, out_channels=256, kernel_size=(1,1), is_max_pool=False, dtype=torch.float64),\n",
        "            CNNBlock(in_channels=256, out_channels=512, kernel_size=(3,3), is_max_pool=False, dtype=torch.float64),\n",
        "            CNNBlock(in_channels=512, out_channels=256, kernel_size=(1,1), is_max_pool=False, dtype=torch.float64),\n",
        "            CNNBlock(in_channels=256, out_channels=512, kernel_size=(3,3), is_max_pool=False, dtype=torch.float64),\n",
        "            CNNBlock(in_channels=512, out_channels=512, kernel_size=(1,1), is_max_pool=False, dtype=torch.float64),\n",
        "            CNNBlock(in_channels=512, out_channels=1024, kernel_size=(3,3), is_max_pool=True, dtype=torch.float64)\n",
        "        )\n",
        "        \n",
        "        '''self.con5 = nn.Sequential(\n",
        "            CNNBlock(in_channels=1024, out_channels=512, kernel_size=(1,1), dtype=float),\n",
        "            CNNBlock(in_channels=512, out_channels=1024, kernel_size=(3,3), dtype=float),\n",
        "            CNNBlock(in_channels=1024, out_channels=512, kernel_size=(1,1), dtype=float),\n",
        "            CNNBlock(in_channels=512, out_channels=1024, kernel_size=(3,3), dtype=float),\n",
        "            CNNBlock(in_channels=1024, out_channels=1024, kernel_size=(1,1), dtype=float),\n",
        "            CNNBlock(in_channels=1024, out_channels=1024, kernel_size=(3,3), stride=2, dtype=float)\n",
        "            )'''\n",
        "        \n",
        "        self.con5 = nn.Sequential(\n",
        "            CNNBlock(in_channels=1024, out_channels=1024, kernel_size=(1,1), dtype=float),\n",
        "            CNNBlock(in_channels=1024, out_channels=1024, kernel_size=(3,3), dtype=float)\n",
        "        )\n",
        "\n",
        "        self.con6 = nn.Sequential(\n",
        "            CNNBlock(in_channels=1024, out_channels=1024, kernel_size=(1,1), dtype=float),\n",
        "            CNNBlock(in_channels=1024, out_channels=1024, kernel_size=(1,1), dtype=float)\n",
        "        )\n",
        "\n",
        "        self.flatted = nn.Flatten()\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(in_features=9216, out_features=2048, dtype=float),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(in_features=2048, out_features=(7 * 7) * ((5 * 2) + 3), dtype=float)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.con1(x)\n",
        "        print('ok')\n",
        "        x = self.con2(x)\n",
        "        print('ok')\n",
        "        x = self.con3(x)\n",
        "        print('ok')\n",
        "        x = self.con4(x)\n",
        "        print('ok')\n",
        "        x = self.con5(x)\n",
        "        print('ok')\n",
        "        x = self.con6(x)\n",
        "        print('ok')\n",
        "        x = self.flatted(x)\n",
        "        print('ok')\n",
        "        x = self.linear(x)\n",
        "        print('ok')\n",
        "        \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_model = YOLO()\n",
        "#temp_model.to(device)\n",
        "temp_model.eval()\n",
        "expected_output_shape = temp_model.S * temp_model.S * (5 * temp_model.B + temp_model.C)\n",
        "\n",
        "test_image = train_dataset[0][0]\n",
        "test_image = test_image[None, :]\n",
        "\n",
        "assert temp_model(test_image).reshape(-1).shape[0] == train_dataset[0][1].reshape(-1).shape[0]\n",
        "assert temp_model(test_image).reshape(-1).shape[0] == expected_output_shape"
      ],
      "metadata": {
        "id": "dkwNV-NUXMkh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e971cd5e-60b4-49e8-8952-226cd93c4d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(349, 349, 3)\n",
            "[[0.48567335243553006, 0.5214899713467048, 0.9255014326647565, 0.9570200573065902]]\n",
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n",
            "(349, 349, 3)\n",
            "[[0.48567335243553006, 0.5214899713467048, 0.9255014326647565, 0.9570200573065902]]\n",
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n",
            "ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJIjWKbcYUYe"
      },
      "source": [
        "## Реализуйте YoloLoss - 3 балла"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwJZ7o0BYTbB"
      },
      "outputs": [],
      "source": [
        "class YoloLoss(nn.Module):\n",
        "    def __init__(self, S=7, B=2, C=3):\n",
        "        \"\"\"\n",
        "        :param: S * S - количество ячеек на которые разбивается изображение\n",
        "        :param: B - количество предсказанных прямоугольников в каждой ячейке\n",
        "        :param: C - количество классов\n",
        "        \"\"\"\n",
        "        \n",
        "        super().__init__()\n",
        "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
        "\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "\n",
        "        self.lambda_noobj = 0.5\n",
        "        self.lambda_coord = 5\n",
        "\n",
        "    def intersection_over_union_small1(mlist, i, j, b = self.S):\n",
        "      \n",
        "      x = mlist[0]\n",
        "      y = mlist[1]\n",
        "      w = mlist[2]\n",
        "      h = mlist[3]\n",
        "\n",
        "      x_min = (i / b + mlist[0] / b) - mlist[2] / 2\n",
        "      y_min = (j / b + mlist[1] / b) - mlist[3] / 2\n",
        "      x_max = (i / b + mlist[0] / b) + mlist[2] / 2\n",
        "      y_max = (j / b + mlist[1] / b) + mlist[3] / 2\n",
        "\n",
        "      f = [x_min, y_min, x_max, y_max]\n",
        "\n",
        "      return f\n",
        "\n",
        "    def intersection_over_union_small2(mlist, i, j, b = self.S):\n",
        "      \n",
        "      x = mlist[0]\n",
        "      y = mlist[1]\n",
        "      w = mlist[2]\n",
        "      h = mlist[3]\n",
        "\n",
        "      x_min = (i / b + mlist[5] / b) - mlist[7] / 2\n",
        "      y_min = (j / b + mlist[6] / b) - mlist[8] / 2\n",
        "      x_max = (i / b + mlist[5] / b) + mlist[7] / 2\n",
        "      y_max = (j / b + mlist[6] / b) + mlist[8] / 2\n",
        "\n",
        "      f = [x_min, y_min, x_max, y_max]\n",
        "\n",
        "      return f\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, predictions, target):\n",
        "\n",
        "        predictions = torch.reshape(predictions, (-1, self.S, self.S, (5 * self.B) + self.C))\n",
        "        all_loss = 0\n",
        "\n",
        "        for batch in range(predictions.shape[0]):\n",
        "            loss1 = 0\n",
        "            loss2 = 0\n",
        "            loss3 = 0\n",
        "            loss4 = 0\n",
        "            loss5 = 0\n",
        "            for i in range(self.S):\n",
        "              for j in range(self.S):\n",
        "                if intersection_over_union(intersection_over_union_small1(predictions, i, j, self.S), intersection_over_union_small1(target, i, j, self.S)) > intersection_over_union(intersection_over_union_small2(predictions, i, j, self.S), intersection_over_union_small2(target, i, j, self.S)):\n",
        "                  needed_box = predictions[0:5]\n",
        "                  nonimp_box = predictions[5:10]\n",
        "                else:\n",
        "                  needed_box = predictions[5:10]\n",
        "                  nonimp_box = predictions[0:5]\n",
        "                  \n",
        "                loss1 += (needed_box[0] - target[0]) ** 2 + (needed_box[1] - target[1]) ** 2\n",
        "\n",
        "                loss2 += (math.sqrt(needed_box[2]) - math.sqrt(target[2])) ** 2 + (math.sqrt(needed_box[3]) - math.sqrt(target[3])) ** 2\n",
        "                \n",
        "\n",
        "                loss3 += (needed_box[4] - intersection_over_union(needed_box[:4], target[:4])) ** 2\n",
        "\n",
        "                loss4 += nonimp_box[4]**2\n",
        "\n",
        "                loss5 += (predictions[10]-box_target[10])**2+(predictions[11]-target[11])**2+(predictions[12]-target[12])**2\n",
        "            \n",
        "                if target[10] != 0 or target[11] != 0 or target[12] != 0:\n",
        "                  gigaloss = self.lambda_coord * loss1 + self.lambda_coord * loss2 + loss3 + self.lambda_noobj * loss4 + loss5\n",
        "                else:\n",
        "                  gigaloss = self.lambda_noobj * loss4\n",
        "         \n",
        "         \n",
        "         return gigaloss\n",
        "\n",
        "        pass\n",
        "\n",
        "        #С новым годом"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}